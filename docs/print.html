<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Forgotten Bits: A Programmer&#x27;s Guide to SE + AI Wisdom</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Forgotten Bits: A Programmer&#x27;s Guide to SE + AI Wisdom</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="display: flex;"
  <center>
    <img width=600 src="img/saillite.png">
  </center>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="back-cover"><a class="header" href="#back-cover">Back Cover</a></h1>
<blockquote>
<p>In a world of AI autopilot, this book teaches you how to take the wheel—by getting to the bottom of things, not just skimming the top.</p>
</blockquote>
<p>AI does the work.
But do we still understand it?
Can we debug, adapt, build from scratch?
Or are we now just passengers?</p>
<p>I see it every day—
students lost in the basics.
Files? Configs? Crashes? Blank stares.</p>
<p>This book brings back the core skills.
Not anti-AI but pro-agency.
So if you want to
build with understanding, read this book.</p>
<h2 id="totally-real-reviews"><a class="header" href="#totally-real-reviews">Totally Real Reviews</a></h2>
<blockquote>
<p><em>"Turns out, the answer wasn’t 'add more layers'."</em><br />
— <strong>Ike Newton</strong>, Warden, <strong>Royal Mint Systems</strong></p>
</blockquote>
<blockquote>
<p><em>"This book is the antidote to the collective amnesia setting in."</em><br />
— <strong>Chuck Babbage</strong>, Professor, <strong>Cambridge University, UK</strong>**</p>
</blockquote>
<blockquote>
<p><em>"Finally, a book that dares to whisper what we've all screamed into the void."</em><br />
— <strong>Gracie Hopper</strong>, Software Project Manager, <strong>US Navy, USA</strong></p>
</blockquote>
<blockquote>
<p><em>"Every page is a gentle slap reminding us that AI isn’t magic—it’s math wrapped in marketing."</em><br />
— <strong>Al Turing</strong>, Director, Computing, <strong>University of Manchestor, UK</strong></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-forgotten-bits"><a class="header" href="#the-forgotten-bits">The Forgotten Bits</a></h1>
<p>In this AI-powered age, the tools are dazzling.
Code writes itself. Systems configure themselves. Everything is faster, easier, more accessible.
And that’s a win—for most. Now,
more people than ever can build more things and ship them sooner. That’s democratic. That’s powerful.</p>
<p>But there’s a cost.
Surgeons don’t learn by turning their backs on patients and browsing web pages. They learn by reaching in—by touching what they do—so they know where to go when something breaks.
Similarly, if you're training to be a software engineer
(or working at the edge of research)  you can’t just drive the car. You need to know what’s under the hood.</p>
<p>My students struggle with this, all the time.</p>
<ul>
<li>They’ve never configured a system from scratch.</li>
<li>They can’t trace a bug through layers of automation.</li>
<li>They’re drifting—out of the creative loop, out of the technical conversation.</li>
<li>They're forgetting how to touch the machine.</li>
</ul>
<p>That’s why this book exists. To bring back the foundations. To train people who not only use tools, but understand them.
Science and engineering demand that kind of contact for  reproducibility, transparency, accountability.
You can’t debug a black box. You can’t verify results you don’t understand. You can’t improve what you’ve never seen inside.</p>
<p>AI tools are brilliant at hiding the mess. But to build trustworthy systems—or critique and improve them—we need people who still know how to see the mess, step through it, and make sense of it.
As Donald Knuth once put it:</p>
<blockquote>
<p>"Email is a wonderful thing for people whose role in life is to
be on top of things. But not for me; my role is to be on the
<strong>bottom of things</strong>. What I do takes long hours of studying and uninterruptible
concentration. I try to learn certain areas of computer science
exhaustively; then I try to digest that knowledge into a form that
is accessible to people who don't have time for such study. The
<strong>transfer of information from the bottom to the top</strong> is what I do for
a living."</p>
</blockquote>
<p>That’s the spirit behind this book.</p>
<p>It’s not anti-AI. It’s pro-agency, pro-curiosity, pro-understanding.<br />
It’s about staying connected to the craft, even as the tools get slicker.<br />
Let’s not forget how to touch the machine.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="about-the-author"><a class="header" href="#about-the-author">About the Author</a></h1>
<p>Tim Menzies (Ph.D. UNSW 1995, ACM Fellow, IEEE Fellow, ASE Fellow)
is a globally recognized leader in software engineering research,
best known for his pioneering work in data-driven, explainable, and
minimal AI for software systems. Over the past two decades, his
contributions have redefined defect prediction, effort estimation,
and multi-objective optimization, emphasizing transparency and
reproducibility. </p>
<p>As the co-creator of the PROMISE repository, Tim helped establish
modern empirical software engineering, showing that small, interpretable
AI models can outperform larger, more complex ones. Currently, he
works as a full Professor in computer science at NC State, USA. He
is the director of the Irrational Research lab (mad scientists
r'us). His research has earned nearly $20 million in funding from
agencies such as NSF, DARPA, and NASA, as well as from private
companies like Meta, Microsoft and IBM. </p>
<p>Tim has published over 300 papers, with more than 24,000 citations,
and advised 24 Ph.D. students. He is the editor-in-chief of the
editor-in-chief of the Automated Software Engineering journal and
an associate editor for IEEE TSE. His work continues to shape the
future of software engineering, focusing on creating AI tools that
are not only intelligent but also fair, transparent, and trustworthy.</p>
<p>For more information, visit http://timm.fyi.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="easier-ai"><a class="header" href="#easier-ai">Easier AI</a></h1>
<p>This book aims to show a simpler way to do many things.
A repeated result is that learners learn better if they can pick their own training data.
By reflecting on what has  been learned so far, they can avoid the confusing, skip over the redundancies,
and focus on the important parts of the data.</p>
<p>This technique is called <a href="0/refs.html#settles2009">active learning</a>.
And it can be extraordinary effective.
For example, in software engineering (SE), systems
often exhibits "funneling": i.e. despite internal
complexity, software
<a href="0/refs.html#menzies2007a">behavior converges to few outcomes</a>,
enabling
<a href="0/refs.html#lustosa2025">simpler reasoning</a>.  Funneling  explains how my imple BareLogic"  active
learner can build
models using  very little data
for (e.g.)  63   SE multi-objective optimization tasks from the
<a href="0/refs.html#menziesmoot">MOOT repository</a>.
These   tasks are quite diverse and include</p>
<ul>
<li>software process decisions,</li>
<li>optimizing configuration parameters,</li>
<li>tuning learners for better analytics.</li>
</ul>
<p>Successful
for this MOOT problems results (e.g.) better advice for project managers, better control of
software options, and enhanced analytics from learners that are
better tuned to the local data.</p>
<p>MOOT includes 100,000s of examples with up to  a thousand settings.
Each example is labelled with up to five effects.   BareLogic's task is to find the best example(s),
after requesting the   least number of labels .  To do this,
BareLogic labels  <code>N=4</code> rest examples, then it:</p>
<ol>
<li>Scores and sorts labeled examples by "distance to heaven" (where
"heaven" is the ideal target for optimization, e.g., weight=0, mpg=max)</li>
<li>Splits the sort into <code>sqrt(𝑁)</code> <code>best</code> and <code>𝑁−sqrt(𝑁)</code> <code>rest</code> examples.</li>
<li>Trains a two-class Bayes classifier on the <code>best</code> and <code>rest</code> sets.</li>
<li>Finds the unlabeled example <code>X</code> that is most likely <code>best</code> via<br />
<code>argmax(x):log(like(best|X)) - log(log(rest|X))</code></li>
<li>Labels <code>X</code>, then increments <code>N</code>.</li>
<li>If <code>X</code> &lt; <code>Stop</code> then loop back to step1. Else return <code>most</code> (the <code>best[0]</code> item) and a regression tree built from the labeled examples.</li>
</ol>
<p>BareLogic was written for teaching purposes as a simple demonstrator
of   active learning. But in a result consistent with "funneling",
this
quick-and-dirty tool achieves near optimal results using   a
handful of labels.
As shown by  the histogram, right-hand-side of this figure, across 63
tasks:</p>
<ul>
<li>eight labels yielded 62% of the optimal result;</li>
<li>16 labels reached nearly 80%,</li>
<li>32 labels approached 90% optimality,</li>
<li>64 labels barely improves on 32 labels,</li>
<li>etc.</li>
</ul>
<center>
<img src="0/../img/63.png">
<b>
Figure: 20 runs of BareLogic on 63 multi-objective tasks. <br>  
Histogram shows mean <tt>(1 − (most − b4.min)/(b4.mu − b4.min))</tt>.   
<tt>Most</tt>
is the best example returned by BareLogic.   
 <tt>b4</tt> are the untreated
examples.   
<tt>min</tt> is the optimal example closest to heaven.</b>
</center>
<p>The lesson here is that achieving state-of-the-art results can ne
achieved with smarter questioning, not planetary-scale computation (i.e. big AI tools like large language models, or LLMs).
Active learning addresses many common  concerns about AI such as slow
training times, excessive energy needs, esoteric hardware requirements,
testability, reproducibility, and  explainability.</p>
<ul>
<li>The above figure  was created without billions of parameters.
Active learners
need no vast pre-existing knowledge or massive datasets, avoiding
the colossal energy and specialized hardware demands of large-scale
AI.</li>
<li>Further, unlike LLMs where testing is slow and often irreproducible,
BareLogic's Bayesian active learning is fast (e.g., for 63 tasks
and 20 repeated trials, this figure was  generated in three minutes on
a standard   laptop).</li>
<li>Most importantly, active learning  fosters
human-AI partnership.
<ul>
<li>Unlike opaque LLMs, BareLogic's results are
explainable via small labeled sets (e.g., <code>N=32</code>).
Whenever a label is required, humans can understand
and guide the reasoning.</li>
<li>The resulting tiny regression tree models offer
concise, effective, and generalizable insights.</li>
</ul>
</li>
</ul>
<p>Active learning provides a compelling alternative to sheer scale in AI.</p>
<ul>
<li>Its ability to deliver rapid, efficient, and transparent results fundamentally
questions the "bigger is better" assumption dominating current thinking
about AI.</li>
<li>It tell us that intelligence requires more than just size.</li>
<li>I am not the only one proposing weight loss for AI.
<ul>
<li>The success of
<a href="0/refs.html#Zeming2024">LLM distillation</a> (shrinking huge models for specific purposes) shows that
giant models are not always necessary.</li>
<li>Active learning pushes this idea even
further, showing that leaner, smarter modeling can achieve great results.</li>
<li>So
why not, before we build the behemoth, try something smaller and faster?</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-the-software"><a class="header" href="#install-the-software">Install the Software</a></h1>
<p>To demonstrate the message of this book, we use the <code>BareLogic</code> tool.</p>
<h2 id="what-to-instakk-first"><a class="header" href="#what-to-instakk-first">What to Instakk First</a></h2>
<div class="table-wrapper"><table><thead><tr><th style="text-align: center">essential</th><th style="text-align: left">what</th><th style="text-align: left">notes</th></tr></thead><tbody>
<tr><td style="text-align: center">✔</td><td style="text-align: left">bash</td><td style="text-align: left">e.g. inside VSCode, or in googlecode, or on a terminal in Linux or Mac, or the WSL for windows</td></tr>
<tr><td style="text-align: center">✔</td><td style="text-align: left">a good code editor</td><td style="text-align: left">e.g.  vscode, or nvim (but consider the merits of something very lightweight like micro)</td></tr>
<tr><td style="text-align: center">✔</td><td style="text-align: left">Python</td><td style="text-align: left">version 3.13 (or later)</td></tr>
<tr><td style="text-align: center">✔</td><td style="text-align: left">Git</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: center">✔</td><td style="text-align: left">Gawk</td><td style="text-align: left">or awk, version 5 or later</td></tr>
<tr><td style="text-align: center">❌</td><td style="text-align: left">htop</td><td style="text-align: left">or some other cpu minitor</td></tr>
<tr><td style="text-align: center">❌</td><td style="text-align: left">plylint</td><td style="text-align: left">or some other static linter</td></tr>
<tr><td style="text-align: center">❌</td><td style="text-align: left">docco</td><td style="text-align: left">or some other simple documentation generator</td></tr>
</tbody></table>
</div>
<h2 id="install-code-and-data-sets"><a class="header" href="#install-code-and-data-sets">Install code and data sets</a></h2>
<p>Now you need some code and sample data sets</p>
<ol start="0">
<li>
<p>In a new directory, chack out our test data</p>
<pre><code>mkdir newDir # call it anything you like
cd newDir
git clone http://github.com/timm/moot
# do not change directories. Goto step 1
</code></pre>
</li>
<li>
<p>Go to <a href="https://github.com/timm/barelogic">https://github.com/timm/barelogic</a> and click the <strong>Fork</strong> button.</p>
</li>
<li>
<p>Clone your fork to your local machine:</p>
<pre><code>git clone https://github.com/timm/barelogic.git
cd barelogic
</code></pre>
</li>
<li>
<p>Fetch all branches and check out the <code>v0.6</code> branch:</p>
<pre><code>git fetch origin
git checkout -b v0.6 origin/v0.6
</code></pre>
</li>
<li>
<p>Create a new branch for your changes.</p>
<pre><code>git checkout -b my-feature
</code></pre>
</li>
<li>
<p>Test your install (see below)</p>
</li>
<li>
<p>Now you can start working. Make your edits in this directory. Frequently, push your changes to your on-line repo</p>
<pre><code>git add . # add waht ever is new
git commit -m "Describe your changes"
git push origin my-feature
</code></pre>
</li>
</ol>
<h2 id="test-your-install"><a class="header" href="#test-your-install">Test your install</a></h2>
<h3 id="are-you-running-python3"><a class="header" href="#are-you-running-python3">Are you running Python3?</a></h3>
<pre><code class="language-bash">python3 --version
</code></pre>
<p>You should see <code>Python 3.13</code> (or higher).</p>
<h3 id="is-your-data-in-the-right-place"><a class="header" href="#is-your-data-in-the-right-place">Is your data in the right place?</a></h3>
<pre><code class="language-bash">cd barelogic/src
make stats
</code></pre>
<p>If this works, you should see something like this:</p>
<pre><code class="language-bash">    x      y   rows
------ ------ ------
     3      1    197 ../../moot/optimize/config/wc+rs-3d-c4-obj1.csv
     3      1    197 ../../moot/optimize/config/wc+sol-3d-c4-obj1.csv
     3      1    197 ../../moot/optimize/config/wc+wc-3d-c4-obj1.csv
...
</code></pre>
<p>If this test fails, check you have installed gawk  and that the data is in the right place; i.e. from the src directory, ../../moot/optimize</p>
<h3 id="is-your-active-learner-working"><a class="header" href="#is-your-active-learner-working">Is your active learner working?</a></h3>
<pre><code class="language-bash">cd barelogic/src
python3 -B bl.py --quick | column -s, -t
</code></pre>
<p>This will run an experiments. 30 times it will run the active learner using 8,16,20,30,40 samples, then statistically compare
the results. Best results will be marked with an <code>a</code>; second best <code>b</code>. and so on.</p>
<pre><code class="language-bash">#['rows'   'lo'     'x'   'y'   'ms'   'b4'        40          20          16          8           'name']
[398       '0.17'   4     3     5      'c 0.56 '   'a 0.17 '   'a 0.21 '   'a 0.24 '   'b 0.26 '   'auto93']
</code></pre>
<p>Notice that everything is "a" from 16 samples and up. This is to say that there was no win here above 16 sampples.</p>
<h3 id="is-your-tree-learner-working"><a class="header" href="#is-your-tree-learner-working">Is your tree learner working?</a></h3>
<pre><code class="language-bash">cd barelogic/src
python3 -B bl.py --tree
</code></pre>
<p>This test does the active learning (with 32 samples) then builds a tree from the labeled data.</p>
<pre><code class="language-bash">auto93.csv
o{:mu1 0.556 :mu2 0.265 :sd1 0.162 :sd2 0.064}
 d2h  win    n
---- ---- ----
0.50   -4   32
0.43   13   27    Volume &lt;= 350
0.39   25   19    |  Model &gt;  80
0.30   45    2    |  |  origin == 2 ;
0.36   31   13    |  |  origin == 3
0.29   49    2    |  |  |  Volume &lt;= 85 ;
0.37   28   11    |  |  |  Volume &gt;  85
0.36   31    7    |  |  |  |  Volume &lt;= 91
0.36   31    5    |  |  |  |  |  Model &gt;  81 ;
0.36   30    2    |  |  |  |  |  Model &lt;= 81 ;
0.39   23    4    |  |  |  |  Volume &gt;  91
0.39   24    2    |  |  |  |  |  Volume &lt;= 107 ;
0.40   21    2    |  |  |  |  |  Volume &gt;  107 ;
0.52   -8    4    |  |  origin == 1
0.44   12    2    |  |  |  Volume &gt;  232 ;
0.60  -28    2    |  |  |  Volume &lt;= 232 ;
0.55  -15    8    |  Model &lt;= 80
0.52   -8    6    |  |  Volume &lt;= 119
0.43   13    3    |  |  |  Clndrs &gt;  3 ;
0.60  -28    3    |  |  |  Clndrs &lt;= 3 ;
0.64  -38    2    |  |  Volume &gt;  119 ;
0.86  -92    5    Volume &gt;  350
0.84  -88    2    |  Volume &lt;= 440 ;
0.87  -95    3    |  Volume &gt;  440 ;
</code></pre>
<p>Here:</p>
<ul>
<li><code>n</code> is the number of rows in each branch;</li>
<li><code>;</code> denotes a leaf;</li>
<li><code>d2h</code> is the distance of the mean score of the rows in each branch to an optimal zero point (so lower numbers are better)</li>
<li><code>win</code> normalizes d2h as <code>100 - 100 * int(1 - (d2h-min)/(mu-min))</code> (so higher numbers are better and 100 is best).</li>
</ul>
<h3 id="is-your-tree-learner-working-on-all-data-sets"><a class="header" href="#is-your-tree-learner-working-on-all-data-sets">Is your tree learner working, on all data sets?</a></h3>
<pre><code class="language-bash">cd barelogic/src
time make trees
</code></pre>
<p>This will run the active learner (with 32 samples), then the tree learner, on all data sets.  On my machine (with 10 coy cores) this takes under a minute.
The thing to check here is that there are no crashes.</p>
<h3 id="how-good-are-those-trees"><a class="header" href="#how-good-are-those-trees">How good are those trees?</a></h3>
<p>This final test will launch 250 Python processes. So shut down everytime else you are doing before trying this. And it freezes your computer, just do a reboot.</p>
<pre><code class="language-bash">cd barelogic/src
time make aftersReport
</code></pre>
<p>This takes  five minutes to run n my 10 core machine.
If it works, then it prints a little report showing how good are the trees learned from 2,30,40,50 samples (selected
by active learning) at selecting for good examples in the unlabeled space.</p>
<p>In that second round sampling, if you all 20 additional samples then:</p>
<pre><code class="language-bash">samples + additional 10 30 50  70  90
-------   ---------- -- -- -- --- ---
     50     20       67 94 98 100 100
     40     20       70 91 97 100 100
     30     20       71 89 96 100 100
     20     20       72 86 95 100 100

    256      1      -14 33 68  78  92
    128      1       22 56 71  85  97
     64      1       12 55 70  79  95
     32      1       15 43 56  76  95
     16      1      110 31 46  67  89
      8      1        7 15 28  38  58
</code></pre>
<p>This report says that (say) after 64 initial
samples, the 1 more from the tree, these tries select for examples in the unlabeled space that are 70% (median) of the way to optimal. Which is pretty amazing.</p>
<p>Further, in terms of predicting future labels, there is little win after 64 initial samples.</p>
<h2 id="pull-requests"><a class="header" href="#pull-requests">Pull requests</a></h2>
<p>If you do something really cool, or if you fix a bug in my code, I will ask you for a pull request</p>
<ul>
<li>On GitHub, go to your fork and click <strong>"Compare &amp; pull request"</strong>.</li>
<li>Set the pull request’s base to <code>timm/barelogic</code>, branch <code>v0.5</code>, and compare it with <code>YOUR_USERNAME/my-feature</code>.</li>
<li>Add a descriptive title and message, then click <strong>"Create pull request"</strong>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tutorial-using-blpy-for-multi-objective-reasoning--active-learning"><a class="header" href="#tutorial-using-blpy-for-multi-objective-reasoning--active-learning">Tutorial: Using <code>bl.py</code> for Multi-Objective Reasoning &amp; Active Learning</a></h1>
<p>This tutorial explains how to use the <code>bl.py</code> script for finding "good" solutions in your data when you have multiple, possibly conflicting, goals. We'll also see how <strong>active learning</strong> helps achieve this by looking at very few examples. We'll use the <code>auto93.csv</code> dataset, which is about cars.</p>
<h2 id="1-the-goal-finding-the-best-cars-with-multiple-objectives"><a class="header" href="#1-the-goal-finding-the-best-cars-with-multiple-objectives">1. The Goal: Finding the "Best" Cars with Multiple Objectives</a></h2>
<p>Imagine you're looking for a car. You probably don't want just <em>any</em> car. You want one that's good in several ways:</p>
<ul>
<li>Low weight (<code>Lbs-</code> in <code>auto93.csv</code>)</li>
<li>Good acceleration (<code>Acc+</code>)</li>
<li>High miles per gallon (<code>Mpg+</code>)</li>
</ul>
<p>The challenge is that these goals often conflict. A super lightweight car might not be the fastest, and a fuel-efficient car might have slower acceleration. This is where <strong>multi-objective reasoning</strong> comes in: finding items that offer the best overall balance or trade-off across these different criteria.</p>
<p>The <code>bl.py</code> script helps with this by looking at how you've named your columns in the CSV file. A <code>-</code> at the end means "minimize this," and a <code>+</code> means "maximize this."</p>
<h2 id="2-how-blpy-measures-goodness-the-ydist-function"><a class="header" href="#2-how-blpy-measures-goodness-the-ydist-function">2. How <code>bl.py</code> Measures "Goodness": The <code>ydist</code> Function</a></h2>
<p>To compare different cars, <code>bl.py</code> needs a single score that summarizes how well a car meets all your objectives. This is done by an <strong>aggregation function</strong> called <code>ydist</code>.</p>
<p>Here's how <code>ydist</code> works conceptually:</p>
<ol>
<li><strong>Normalization</strong>: It takes values from different scales (like weight in pounds and MPG) and converts them to a common 0-to-1 scale.</li>
<li><strong>Distance from Ideal</strong>: For each of your goals (e.g., <code>Lbs-</code>, <code>Acc+</code>, <code>Mpg+</code>), it measures how far the car's normalized value is from the "perfect" score (0 for minimization goals like <code>Lbs-</code>, and 1 for maximization goals like <code>Acc+</code> and <code>Mpg+</code>).</li>
<li><strong>Combined Score</strong>: It then combines these individual "distances from ideal" into one overall <code>ydist</code> score. <strong>A lower <code>ydist</code> is better</strong>, indicating the car is closer to the ideal across all your stated objectives.</li>
</ol>
<p>To see this in action, you can run <code>bl.py</code> with an example command that processes your data and shows these distances. The <code>eg-data</code> command will load a CSV and print statistics for each column, including objective columns which are the basis for <code>ydist</code>.</p>
<pre><code class="language-bash">python bl.py --data -f data/auto93.csv
</code></pre>
<p>This command processes <code>auto93.csv</code> and shows, among other things, the characteristics of the objective columns (<code>y</code> columns) from which <code>ydist</code> is calculated. While it doesn't directly output the <code>ydist</code> for each row via cthis specific command, the script contains <code>eg-ydist</code> (run via <code>test/ydata.py</code>) that does show this.</p>
<h2 id="illustrative-output-conceptual-if-eg-ydist-were-run-showing-best-and-worst"><a class="header" href="#illustrative-output-conceptual-if-eg-ydist-were-run-showing-best-and-worst">Illustrative Output (Conceptual, if <code>eg-ydist</code> were run, showing best and worst):</a></h2>
<pre><code>Clndrs  Volume  HpX  Model  origin  Lbs-  Acc+  Mpg+  ydist
------  ------  ---  -----  ------  ----  ----  ----  -----
4       90      48   80     2       2085  21.7  40    0.17
4       91      67   80     3       1850  13.8  40    0.17
4       86      65   80     3       2019  16.4  40    0.17
4       86      64   81     1       1875  16.4  40    0.18
...
8       429     198  70     1       4341  10.0  20    0.77
8       350     165  70     1       3693  11.5  20    0.77
8       455     225  70     1       4425  10.0  10    0.77
8       454     220  70     1       4354  9.0   10    0.79
</code></pre>
<h2 id="active-learning-smarter-searching-fewer-labels-with-actlearn"><a class="header" href="#active-learning-smarter-searching-fewer-labels-with-actlearn">Active Learning: Smarter Searching, Fewer Labels with actLearn</a></h2>
<p>Manually checking every single car in a large dataset (i.e., "labeling" it with its performance metrics) can be very time-consuming or expensive. Active learning is a technique to find the best items by looking at only a small, intelligently chosen subset of the data.</p>
<p><code>bl.py</code> uses the <code>actLearn</code> function for this. Here's the core idea:</p>
<ul>
<li><strong>Tiny Start:</strong> It begins by evaluating a very small number of randomly selected cars (e.g., 4 cars, controlled by the <code>-s</code> or <code>--start</code> flag).</li>
<li><strong>Learn and Query:</strong> Based on what it learns from these initial cars, it decides which unlabeled car would be most informative to evaluate next. It does this by:</li>
<li>Maintaining a small set of the <code>best</code> cars found so far and a <code>rest</code> set.</li>
<li>For new candidate cars, it calculates how <code>like</code>ly they are to belong to the i
<code>best</code> set versus the <code>rest</code> set (using a simplified Naive Bayes approach).</li>
<li>It then uses an "acquisition strategy" (set by <code>-a</code> or <code>--acq</code>, e.g., <code>xploit</code> to focus on known good areas, <code>xplore</code> to investigate uncertain areas) to pick the next car to "label" (i.e., calculate its <code>ydist</code>).
Iterate: It repeats this process, adding the newly evaluated car to its knowledge, and refining its understanding of what makes a "good" car.
Stop: It continues until it has evaluated a pre-set small number of cars (e.g., 32 by default, controlled by <code>-S</code> or <code>--Stop</code>).</li>
</ul>
<p>The goal of actLearn is to identify a high-quality set of cars using far fewer evaluations than if you checked every single one.</p>
<h2 id="running-an-active-learning-experiment"><a class="header" href="#running-an-active-learning-experiment">Running an Active Learning Experiment:</a></h2>
<p>You can simulate this process using the <code>--actLearn</code>
command-line flag (which, internally, calles the `eg__actLearn`` function)
and. This will run the active learning strategy and report on how well it performed.</p>
<pre><code class="language-bash">python bl.py --actLearn -f data/auto93.csv -S 32
</code></pre>
<h2 id="interpreting-the-output-from-eg-actlearn"><a class="header" href="#interpreting-the-output-from-eg-actlearn">Interpreting the Output from eg-actLearn:</a></h2>
<p>The script will output an "object" (a summary of results). Key things to look for:</p>
<ul>
<li>win: A score (ideally close to 1.0) indicating how close the best car found by actLearn is to the true best car in the entire dataset (if it were known). A higher "win" means active learning did a good job.</li>
<li>mu1: The ydist score of the best car found by actLearn.
lo0, mu0, hi0: These show the minimum, average, and maximum ydist if all cars were evaluated. This gives you a baseline to compare mu1 against.</li>
<li>stop: The number of cars actLearn actually "labeled" or evaluated (e.g., 32).</li>
<li>ms: The average time in milliseconds per run.</li>
</ul>
<p>An example output might look like:</p>
<pre><code>o{:win 0.87, :rows 398, :x 4, :y 3, :lo0 0.17, :mu0 0.56, :hi0 0.93, :mu1 0.22, :sd1 0.06, :ms 5, :stop 32, :name auto93}
</code></pre>
<p>This (hypothetical) output would mean that by only looking at 32 cars, actLearn found a car with a ydist of 0.22. This is 87% of the way from the average car's score (0.56) towards the best possible score in the dataset (0.17). This demonstrates finding a very good solution with minimal labeling effort.</p>
<h2 id="why-bare-logic"><a class="header" href="#why-bare-logic">Why "Bare Logic"?</a></h2>
<p>bl.py is termed "bare logic" because it implements these AI concepts (data handling, normalization, distance metrics, a simplified Bayesian approach for active learning) from fundamental principles. It avoids large external machine learning libraries, which makes the code:</p>
<ul>
<li>Transparent: Easier to see how calculations are done.</li>
<li>Lightweight: Minimal dependencies.</li>
<li>Educational: Good for understanding the core mechanics.</li>
</ul>
<h2 id="to-experiment-further"><a class="header" href="#to-experiment-further">To Experiment Further:</a></h2>
<ul>
<li>Use the -f flag with python bl.py [example_function] to specify your own CSV data file. For example, python bl.py --actLearn -f path/to/your/data.csv.</li>
<li>Change the -s (start evaluations) and -S (total evaluations) flags to see how performance changes with more or fewer labels. E.g., python bl.py --actLearn -s 10 -S 50.</li>
<li>Test different active learning strategies with the -a flag (e.g., python bl.py --actLearn -a xplore).</li>
<li>Explore other eg-* functions by running python bl.py -h to see what else the script can do. (Note: The example functions are prefixed with eg_ in the code, so on the command line, you'd use --functionName).</li>
</ul>
<p>By using bl.py and its command-line flags, you can perform sophisticated multi-objective reasoning and leverage active learning to find good solutions efficiently, even when data labeling is a constraint.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references"><a class="header" href="#references">References</a></h1>
<h2 id="h"><a class="header" href="#h">H</a></h2>
<h3 id="hou2024"><a class="header" href="#hou2024">Hou,2024</a></h3>
<p>C. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, and et al. 2024.
<a href="https://arxiv.org/pdf/2308.10620">Large Language Models for SE: A Systematic Literature Review</a>. TOSEM 33, 8 (Sept. 2024).</p>
<h2 id="l"><a class="header" href="#l">L</a></h2>
<h3 id="lustosa2025"><a class="header" href="#lustosa2025">Lustosa,2025</a></h3>
<p>A. Lustosa and T. Menzies. 2025.
<a href="https://arxiv.org/pdf/2503.21086">Less Noise, More Signal: DRR for Better Optimizations of SE Tasks</a> arXiv:2503.21086  https://arxiv.org/abs/2503.21086</p>
<h2 id="m"><a class="header" href="#m">M</a></h2>
<h3 id="menzies2007a"><a class="header" href="#menzies2007a">Menzies,2007a</a></h3>
<p>T. Menzies, D. Owen, and J. Richardson. 2007. <a href="https://ieeexplore.ieee.org/iel5/2/4069176/04069195.pdf?casa_token=CYPQ2kPOjmsAAAAA:0jMU2xs4VePD5_6iQi-r2Bo2smb8mvesunTdPqvdcb2WTlYW4hD7DiWwSLaOguUzpfvCkx761g">The Strangest Thing About Software</a>
Computer 40, 1 (2007), 54–60.</p>
<h3 id="menziesbl"><a class="header" href="#menziesbl">Menzies,BL</a></h3>
<p>T. Menzies. 2025. BareLogic Python Source Code. https://github.com/timm/
barelogic/blob/main/src/bl.py.</p>
<h3 id="menziesmoot"><a class="header" href="#menziesmoot">Menzies,MOOT</a></h3>
<p>T. Menzies. 2025. MOOT= Many multi-objective optimization tests. https://github.
com/timm/moot/tree/master/optimize.</p>
<h2 id="p"><a class="header" href="#p">P</a></h2>
<h2 id="pearson1902"><a class="header" href="#pearson1902">Pearson,1902</a></h2>
<p>Pearson, K. (1902). <em>On Lines and Planes of Closest Fit to
Systems of Points in Space</em>. Philosophical Magazine, 6(2), 559-572.
https://doi.org/10.1080/14786440209478072</p>
<h2 id="s"><a class="header" href="#s">S</a></h2>
<h3 id="settles2009"><a class="header" href="#settles2009">Settles,2009</a></h3>
<p>D. Settles. 2009. <a href="https://minds.wisconsin.edu/bitstream/handle/1793/60660/TR1648.pdf?sequence=1&amp;isAllowed=y.">Active learning literature survey</a>. Technical Report 1648. U.
Wisconsin-Madison Dept of CS.</p>
<h2 id="t"><a class="header" href="#t">T</a></h2>
<h3 id="tawosi2023"><a class="header" href="#tawosi2023">Tawosi,2023</a></h3>
<p>V. Tawosi, R. Moussa, and F. Sarro. 2023.
<a href="https://arxiv.org/pdf/2201.05401">Agile Effort Estimation: Have We Solved the Problem Yet?</a> IEEE Trans SE 49, 4 (2023), 2677–2697.</p>
<h2 id="z"><a class="header" href="#z">Z</a></h2>
<h3 id="zeming2024"><a class="header" href="#zeming2024">Zeming,2024</a></h3>
<p>L. Zeming and et al. 2024.
<a href="https://arxiv.org/pdf/2402.13116">A Survey on Knowledge Distillation for Large Language Models</a>. ACM Trans. Int. Systems &amp; Technology (2024).</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
